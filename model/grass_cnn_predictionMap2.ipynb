{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d19d9f-ca34-429f-bd7d-4797891012fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13af245f-7292-4249-a059-8732f80afc9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(11, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 224 -> 112\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 112 -> 56\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 56 -> 28\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 28 -> 14\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c5780e-0679-4d2b-b7ad-2d9e60a13a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmallCNN4(nn.Module):\n",
    "    def __init__(self, input_channels=10, num_classes=2):\n",
    "        super(SmallCNN4, self).__init__()\n",
    "\n",
    "        # First CNN Layer\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample by 2\n",
    "\n",
    "        # Depthwise Separable Convolution\n",
    "        self.depthwise = nn.Conv2d(32, 32, kernel_size=3, padding=1, groups=32)\n",
    "        self.pointwise = nn.Conv2d(32, 64, kernel_size=1)\n",
    "\n",
    "        # Global Average Pooling to (2, 2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(64 * 4 * 4, num_classes)  # 64 channels with spatial size (2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First CNN Layer\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Depthwise Separable Convolution\n",
    "        x = F.relu(self.pointwise(self.depthwise(x)))\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.gap(x).view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5584fdd4-10d7-4735-b28e-057239b39e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make prediction map\n",
    "# Define custom Dataset for the TIFF image\n",
    "\n",
    "\n",
    "# Define custom Dataset for the TIFF image\n",
    "class TiffDataset(Dataset):\n",
    "    def __init__(self, image, tile_size, stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (np.ndarray): Input multi-band image (H, W, C).\n",
    "            tile_size (int): Size of the patches.\n",
    "            stride (int): Stride for sliding window.\n",
    "        \"\"\"\n",
    "        self.image = image\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.tiles = []\n",
    "        \n",
    "        # Generate tiles with sliding window\n",
    "        for y in range(0, image.shape[0] - tile_size + 1, stride):\n",
    "            for x in range(0, image.shape[1] - tile_size + 1, stride):\n",
    "                self.tiles.append((y, x))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized patch of shape (10, tile_size, tile_size).\n",
    "        \"\"\"\n",
    "        y, x = self.tiles[idx]\n",
    "        patch = self.image[y:y + self.tile_size, x:x + self.tile_size, :]\n",
    "\n",
    "        # Extract the bands\n",
    "        blue, green, red, re, nir, swir = patch[..., 0], patch[..., 1], patch[..., 2], patch[..., 3], patch[..., 4], patch[..., 5]\n",
    "\n",
    "        # Calculate indices\n",
    "        # NDVI\n",
    "        ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "        # SAVI (L=0.5)\n",
    "        L = 0.5\n",
    "        savi = ((nir - red) / (nir + red + L)) * (1 + L)\n",
    "        # VARI\n",
    "        vari = (green - red) / (green + red - blue + 1e-6)\n",
    "        # EXG\n",
    "        exg = 2 * green - red - blue\n",
    "        # NDRE\n",
    "        ndre = (nir - re) / (nir + re + 1e-6)\n",
    "\n",
    "        # Stack bands into a 10-band patch\n",
    "        # patch_10band = np.stack([blue, green, red, re, nir, swir, ndvi, savi, vari, exg, ndre], axis=-1)\n",
    "        patch_10band = np.stack([blue, green, red, re, nir, ndvi, savi, vari, exg, ndre], axis=-1)\n",
    "\n",
    "        # Normalize each band independently\n",
    "        patch_10band = (patch_10band - patch_10band.mean(axis=(0, 1), keepdims=True)) / \\\n",
    "                       (patch_10band.std(axis=(0, 1), keepdims=True) + 1e-8)\n",
    "\n",
    "        # Convert to PyTorch tensor and rearrange to (C, H, W)\n",
    "        patch_10band = torch.from_numpy(patch_10band).float().permute(2, 0, 1)\n",
    "        return patch_10band, y, x, savi[self.tile_size // 2, self.tile_size // 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01170156-206d-42bf-881a-825ab1c35e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tiles: 100%|██████████| 7261/7261 [43:58<00:00,  2.75it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction TIFF has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# tiff_src = \"/blue/changzhao/zhou.tang/botanical_composition/data/pad2_july.tif\"\n",
    "# tiff_src = \"/blue/changzhao/zhou.tang/botanical_composition/data/All_Paddock_26_JUL_2024_ortho_bgrent.tiff\"\n",
    "# tiff_src = \"/blue/changzhao/zhou.tang/botanical_composition/data/All_Paddock_22_SEP_2024_ortho_bgrent.tiff\"\n",
    "tiff_src = \"/blue/changzhao/zhou.tang/botanical_composition/data/2023/Paddock_9_July_2023.tif\"\n",
    "# Parameters\n",
    "tile_size = 50\n",
    "stride = 1\n",
    "batch_size = 7200  # Adjust based on GPU memory\n",
    "weight_url = 'best_model_20241221_50_smallmodel4.pth'\n",
    "out_tif_url = 'predictions_pad9_2023July_20241227_smallCNN4_50px_filtersavi0.6.tiff'\n",
    "n_worker = 36\n",
    "savi_thred = 0.6\n",
    "predict_none = 3.0\n",
    "\n",
    "# Load the TIFF image\n",
    "with rasterio.open(tiff_src) as src:\n",
    "    image = src.read()  # Read all bands\n",
    "    transform = src.transform\n",
    "    crs = src.crs\n",
    "\n",
    "# Ensure shape: (H, W, C)\n",
    "image = np.moveaxis(image, 0, -1)\n",
    "\n",
    "# Padding the image\n",
    "pad_h = (image.shape[0] + tile_size - 1) // tile_size * tile_size\n",
    "pad_w = (image.shape[1] + tile_size - 1) // tile_size * tile_size\n",
    "padded_image = np.pad(image, ((0, pad_h - image.shape[0]), (0, pad_w - image.shape[1]), (0, 0)), mode='reflect')\n",
    "\n",
    "# Create the Dataset and DataLoader\n",
    "dataset = TiffDataset(padded_image, tile_size, stride)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=n_worker, pin_memory=True)\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = SimpleCNN(num_classes=2).to(device)\n",
    "# model.load_state_dict(torch.load('best_model_20241116_0.94.pth'))\n",
    "model = SmallCNN4(num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load(weight_url))\n",
    "\n",
    "model.eval().to(device)\n",
    "\n",
    "# Prepare the output prediction array\n",
    "output_predictions = np.zeros((padded_image.shape[0], padded_image.shape[1]), dtype=np.float32)\n",
    "\n",
    "# Compute the offset to place the prediction in the center of the patch\n",
    "center_offset = tile_size // 2\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    for tiles, ys, xs, savis in tqdm(dataloader, desc=\"Processing tiles\"):\n",
    "        tiles = tiles.to(device)  # Move tiles to GPU\n",
    "        tiles = tiles.contiguous()\n",
    "        # print(torch.cuda.memory_summary())\n",
    "        preds = model(tiles)  # Adjust based on your model's output shape\n",
    "        preds = preds.argmax(dim=1)\n",
    "        # print(f\"get {len(ys)} prediction\")\n",
    "\n",
    "        for i in range(len(ys)):\n",
    "            y, x, savi = ys[i].item(), xs[i].item(), savis[i].numpy()\n",
    "            # print(f\"savi is {savi}\")\n",
    "            # print(f\"x is {x}, y is {y}\")\n",
    "            center_y = y + center_offset\n",
    "            center_x = x + center_offset\n",
    "            if savi >= savi_thred:\n",
    "                output_predictions[center_y, center_x] = preds[i].cpu().item()\n",
    "            else:\n",
    "                output_predictions[center_y, center_x] = predict_none\n",
    "                    \n",
    "\n",
    "# Crop the predictions back to the original image size\n",
    "output_predictions = output_predictions[:image.shape[0], :image.shape[1]]\n",
    "\n",
    "# Save the predictions as a new TIFF\n",
    "with rasterio.open(\n",
    "    out_tif_url,\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=output_predictions.shape[0],\n",
    "    width=output_predictions.shape[1],\n",
    "    count=1,\n",
    "    dtype='float32',\n",
    "    crs=crs,\n",
    "    transform=transform,\n",
    ") as dst:\n",
    "    dst.write(output_predictions, 1)\n",
    "\n",
    "print(\"Prediction TIFF has been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soc_gpu",
   "language": "python",
   "name": "soc_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
