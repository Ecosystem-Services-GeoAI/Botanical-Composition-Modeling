{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd28d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved → ./rf_paddock_hold-out_3_repetition_outputs\\split_pad3\\split_pad3_best_model.joblib\n",
      "Best model saved → ./rf_paddock_hold-out_3_repetition_outputs\\split_pad5\\split_pad5_best_model.joblib\n",
      "Best model saved → ./rf_paddock_hold-out_3_repetition_outputs\\split_pad9\\split_pad9_best_model.joblib\n",
      "        Metric   Class     Value\n",
      "0    Precision   grass  0.767943\n",
      "1  Sensitivity   grass  0.973118\n",
      "2  Specificity   grass  0.606349\n",
      "3     F1-score   grass  0.858251\n",
      "4    Precision  legume  0.951804\n",
      "5  Sensitivity  legume  0.606349\n",
      "6  Specificity  legume  0.973118\n",
      "7     F1-score  legume  0.738078\n",
      "Class           grass    legume   Average\n",
      "Metric                                   \n",
      "Precision    0.767943  0.951804  0.859874\n",
      "Sensitivity  0.973118  0.606349  0.789734\n",
      "Specificity  0.606349  0.973118  0.789734\n",
      "F1-score     0.858251  0.738078  0.798165\n",
      "\n",
      "=== Overall Testing Performance (3 independent runs) – Macro Metrics (%) ===\n",
      "                  grass  legume  Average\n",
      "Precision         76.79   95.18    85.99\n",
      "Sensitivity       97.31   60.63    78.97\n",
      "Specificity       60.63   97.31    78.97\n",
      "F1-score          85.83   73.81    79.82\n",
      "Overall Accuracy    NaN     NaN    81.68\n",
      "\n",
      "All outputs saved to: C:\\Users\\changzhao\\UFL Dropbox\\Chang Zhao\\Research\\Botanical_Composition\\Spatial_Partition\\Paddock_Hold-Out\\rf_paddock_hold-out_3_repetition_outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Independent training per file (pad3, pad5, pad9) with RF + GridSearchCV\n",
    "# Aggregate testing performance & feature importance across the three runs\n",
    "# ============================================================\n",
    "import os, warnings, json, joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "feature_cols = ['B','G','R','RE','NIR','NDVI','NDRE','SAVI','VARI','ExG']\n",
    "target_col   = 'class'\n",
    "csv_files    = [\n",
    "    \"split_pad3.csv\",\n",
    "    \"split_pad5.csv\",\n",
    "    \"split_pad9.csv\"\n",
    "]\n",
    "\n",
    "classes_order = [\"grass\", \"legume\"]   # fixed order in CMs & table\n",
    "\n",
    "out_dir = \"./rf_paddock_hold-out_3_repetition_outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Lean but effective grid (10 features, mild imbalance)\n",
    "param_grid = {\n",
    "    \"rf__n_estimators\": [400],          # we’ll refit final with 1000 trees\n",
    "    \"rf__max_depth\": [None, 20, 30],\n",
    "    \"rf__min_samples_split\": [2, 5, 10],\n",
    "    \"rf__min_samples_leaf\": [1, 2, 4],\n",
    "    \"rf__max_features\": [\"sqrt\", 0.5],\n",
    "    \"rf__bootstrap\": [True],\n",
    "}\n",
    "cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "def plot_cm(cm, title, path_png, normalize=False):\n",
    "    if normalize:\n",
    "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "            cm_disp = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "        fmt = \".2f\"\n",
    "    else:\n",
    "        cm_disp = cm\n",
    "        fmt = None\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_disp, display_labels=classes_order)\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format=fmt)\n",
    "    plt.title(title)\n",
    "    plt.yticks(rotation=\"vertical\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def cm_to_metrics(cm):\n",
    "    \"\"\"Return per-class dicts (precision, sensitivity/recall, specificity, f1) and OA from a 2x2 CM.\"\"\"\n",
    "    metrics = {}\n",
    "    total = cm.sum()\n",
    "    for i, cls in enumerate(classes_order):\n",
    "        TP = cm[i, i]\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        TN = total - TP - FN - FP\n",
    "\n",
    "        prec = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        rec  = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "        f1   = (2*prec*rec)/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "        metrics[cls] = dict(precision=prec, sensitivity=rec, specificity=spec, f1=f1)\n",
    "\n",
    "    OA = np.trace(cm) / total if total > 0 else 0.0\n",
    "    return metrics, OA\n",
    "\n",
    "# -------------------------------\n",
    "# One independent run (one file)\n",
    "# -------------------------------\n",
    "def run_one_file(csv_path):\n",
    "    tag = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    run_dir = os.path.join(out_dir, tag)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    train_df = df[df['folder'] == 'training'].copy()\n",
    "    test_df  = df[df['folder'] == 'testing' ].copy()\n",
    "\n",
    "    X_tr, y_tr = train_df[feature_cols], train_df[target_col].astype(str)\n",
    "    X_te, y_te = test_df[feature_cols],  test_df[target_col].astype(str)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"rf\", RandomForestClassifier(\n",
    "            random_state=42, n_jobs=-1, class_weight=\"balanced\"\n",
    "        ))\n",
    "    ])\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, scoring=\"f1_weighted\",\n",
    "                        cv=cv5, n_jobs=-1, verbose=0)\n",
    "    grid.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Save best hyperparameters to JSON\n",
    "    best_params = grid.best_params_.copy()\n",
    "    with open(os.path.join(run_dir, f\"{tag}_best_params.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    # Optional variance reduction: refit best with 1000 trees\n",
    "    from sklearn.base import clone\n",
    "    best_params = grid.best_params_.copy()\n",
    "    best_params[\"rf__n_estimators\"] = 1000\n",
    "    final_model = clone(grid.best_estimator_)\n",
    "    final_model.set_params(**best_params)\n",
    "    final_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Save model to .joblib\n",
    "    model_path = os.path.join(run_dir, f\"{tag}_best_model.joblib\")\n",
    "    joblib.dump(final_model, model_path)\n",
    "    print(f\"Best model saved → {model_path}\")\n",
    "\n",
    "    # Test predictions\n",
    "    y_pred = final_model.predict(X_te)\n",
    "\n",
    "    # Confusion matrices\n",
    "    cm = confusion_matrix(y_te, y_pred, labels=classes_order)\n",
    "    plot_cm(cm, f\"Prediction number\",\n",
    "            os.path.join(run_dir, f\"{tag}_cm_counts.png\"), normalize=False)\n",
    "    plot_cm(cm, f\"Prediction percentage\",\n",
    "            os.path.join(run_dir, f\"{tag}_cm_norm.png\"), normalize=True)\n",
    "\n",
    "    # Metrics from CM\n",
    "    per_class, OA = cm_to_metrics(cm)\n",
    "    # Save per-class metrics for this file\n",
    "    per_file_rows = []\n",
    "    for cls in classes_order:\n",
    "        m = per_class[cls]\n",
    "        per_file_rows.append({\n",
    "            \"Dataset\": tag, \"Class\": cls,\n",
    "            \"Precision\": m[\"precision\"], \"Sensitivity\": m[\"sensitivity\"],\n",
    "            \"Specificity\": m[\"specificity\"], \"F1\": m[\"f1\"]\n",
    "        })\n",
    "    pd.DataFrame(per_file_rows).to_csv(os.path.join(run_dir, f\"{tag}_per_class_metrics.csv\"), index=False)\n",
    "\n",
    "    # Feature importances\n",
    "    rf = final_model.named_steps[\"rf\"]\n",
    "    fi = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "    fi.to_csv(os.path.join(run_dir, f\"{tag}_feature_importances.csv\"), header=[\"importance\"])\n",
    "    plt.figure(figsize=(8,4), dpi=300)\n",
    "    fi.plot(kind='bar')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title(f'Feature Importances')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(run_dir, f\"{tag}_feature_importances.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"best_cv_score\": grid.best_score_,\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"cm\": cm,\n",
    "        \"per_class\": per_class,\n",
    "        \"OA\": OA,\n",
    "        \"fi\": fi,\n",
    "        \"y_true\": y_te.to_numpy(),\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Run all three files independently\n",
    "# -------------------------------\n",
    "results = [run_one_file(p) for p in csv_files]\n",
    "\n",
    "# -------------------------------\n",
    "# Build the overall performance table (your format, in %)\n",
    "# -------------------------------\n",
    "rows = []\n",
    "for cls in classes_order:\n",
    "    prec = np.mean([r[\"per_class\"][cls][\"precision\"]   for r in results])\n",
    "    sens = np.mean([r[\"per_class\"][cls][\"sensitivity\"] for r in results])\n",
    "    spec = np.mean([r[\"per_class\"][cls][\"specificity\"] for r in results])\n",
    "    f1   = np.mean([r[\"per_class\"][cls][\"f1\"]          for r in results])\n",
    "    rows.append({\"Metric\":\"Precision\",   \"Class\":cls, \"Value\":prec})\n",
    "    rows.append({\"Metric\":\"Sensitivity\", \"Class\":cls, \"Value\":sens})\n",
    "    rows.append({\"Metric\":\"Specificity\", \"Class\":cls, \"Value\":spec})\n",
    "    rows.append({\"Metric\":\"F1-score\",    \"Class\":cls, \"Value\":f1})\n",
    "\n",
    "OA_mean = np.mean([r[\"OA\"] for r in results])\n",
    "\n",
    "tbl = pd.DataFrame(rows)\n",
    "print(tbl)\n",
    "pivot = tbl.pivot(index=\"Metric\", columns=\"Class\", values=\"Value\").reindex(\n",
    "    [\"Precision\",\"Sensitivity\",\"Specificity\",\"F1-score\"]\n",
    ")\n",
    "pivot[\"Average\"] = pivot.mean(axis=1)\n",
    "print(pivot)\n",
    "final_table = (pivot * 100).round(2)\n",
    "\n",
    "oa_row = pd.DataFrame({\"grass\":[np.nan], \"legume\":[np.nan], \"Average\":[round(100*OA_mean,2)]},\n",
    "                      index=[\"Overall Accuracy\"])\n",
    "final_table = pd.concat([final_table, oa_row], axis=0)\n",
    "final_table = final_table[[\"grass\",\"legume\",\"Average\"]]\n",
    "\n",
    "print(\"\\n=== Overall Testing Performance (3 independent runs) – Macro Metrics (%) ===\")\n",
    "print(final_table)\n",
    "final_table.to_csv(os.path.join(out_dir, \"overall_metrics_table.csv\"))\n",
    "\n",
    "# Also save per-file summary\n",
    "per_file_summary = []\n",
    "for r in results:\n",
    "    row = {\"Dataset\": r[\"tag\"], \"Best_CV_F1_weighted\": r[\"best_cv_score\"], \"Overall_Accuracy\": r[\"OA\"]}\n",
    "    for cls in classes_order:\n",
    "        m = r[\"per_class\"][cls]\n",
    "        row[f\"{cls}_Precision\"]   = m[\"precision\"]\n",
    "        row[f\"{cls}_Sensitivity\"] = m[\"sensitivity\"]\n",
    "        row[f\"{cls}_Specificity\"] = m[\"specificity\"]\n",
    "        row[f\"{cls}_F1\"]          = m[\"f1\"]\n",
    "    per_file_summary.append(row)\n",
    "pd.DataFrame(per_file_summary).to_csv(os.path.join(out_dir, \"per_file_metrics_raw.csv\"), index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Overall pooled confusion matrix across all three test sets\n",
    "# -------------------------------\n",
    "overall_cm = sum([r[\"cm\"] for r in results])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=overall_cm, display_labels=classes_order)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Prediction number\")\n",
    "plt.yticks(rotation=\"vertical\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"overall_cm_counts.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "    overall_cm_norm = overall_cm.astype(float) / overall_cm.sum(axis=1, keepdims=True)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=overall_cm_norm, display_labels=classes_order)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format=\".2f\")\n",
    "plt.title(\"Prediction percentage\")\n",
    "plt.yticks(rotation=\"vertical\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"overall_cm_norm.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# -------------------------------\n",
    "# Aggregate feature importances (mean ± SD) across runs\n",
    "# -------------------------------\n",
    "fi_df = pd.DataFrame({r[\"tag\"]: r[\"fi\"] for r in results}).T.fillna(0.0)\n",
    "fi_df.to_csv(os.path.join(out_dir, \"feature_importances_by_file.csv\"))\n",
    "fi_mean = fi_df.mean(axis=0).sort_values(ascending=False)\n",
    "fi_std  = fi_df.std(axis=0).reindex(fi_mean.index)\n",
    "\n",
    "agg_fi = pd.DataFrame({\"mean_importance\": fi_mean, \"std_importance\": fi_std})\n",
    "agg_fi.to_csv(os.path.join(out_dir, \"feature_importances_mean_std.csv\"))\n",
    "\n",
    "plt.figure(figsize=(9,4.5), dpi=300)\n",
    "plt.bar(range(len(fi_mean)), fi_mean.values, yerr=fi_std.values, capsize=3)\n",
    "plt.xticks(range(len(fi_mean)), fi_mean.index, rotation=45, ha='right')\n",
    "plt.ylabel('Mean Importance (± SD)')\n",
    "plt.title('Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"feature_importances_mean_std.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {os.path.abspath(out_dir)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
